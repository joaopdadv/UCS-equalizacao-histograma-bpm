Nos experimentos realizados, tanto OpenMP quanto MPI apresentaram comportamento muito semelhante em termos de speedup e eficiência, indicando que o problema (aplicação do filtro mediana seguido da conversão para tons de cinza) é altamente paralelizável. A eficiência manteve-se entre 70% e 90% nas duas abordagens, mostrando baixo overhead de paralelização. Embora o MPI envolva comunicação explícita entre processos, o custo dessa comunicação foi praticamente desprezível devido ao processamento local predominante e ao fato de todos os processos estarem executando na mesma máquina. Como resultado, o MPI chegou inclusive a apresentar tempos ligeiramente menores que o OpenMP no caso serial, possivelmente devido a menor sobrecarga de gerenciamento interno. Para até quatro unidades de processamento, ambas as abordagens escalam quase identicamente. Contudo, conceitualmente, OpenMP é mais adequado para arquiteturas de memória compartilhada, enquanto MPI tende a escalar melhor que OpenMP em sistemas distribuídos ou com número muito maior de processos. Em resumo, para este problema e neste ambiente de execução, OpenMP e MPI apresentam desempenho praticamente equivalente.

